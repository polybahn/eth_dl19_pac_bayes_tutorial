{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.0\n",
    "!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this tutorial, we will implement a simple linear regression model with PAC Bayesian SGD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "n_sample = 100  # number of samples in training set\n",
    "dim = 5  # dimension of feature vector for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data preparation\n",
    "## generate $X$: $n$ samples of dimension $d$ ~ $\\mathcal{N}(0, diag[\\sigma_1^2, \\sigma_2^2, ..., \\sigma_d^2$])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n"
     ]
    }
   ],
   "source": [
    "mean_x = np.zeros(dim, dtype=np.float32)\n",
    "diag_x = np.zeros([dim, dim], dtype=np.float32)\n",
    "for i, sigma in enumerate(np.sort(np.random.rand(dim))):\n",
    "    diag_x[i, i] = sigma ** 2\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "[[-0.07773376  0.01118072  0.06995384  0.10453022 -0.83898637]\n",
      " [-0.08650132 -0.00944478 -0.06890632 -0.2147365  -0.58603212]]\n"
     ]
    }
   ],
   "source": [
    "# generate X\n",
    "X = np.random.multivariate_normal(mean_x, diag_x, n_sample)\n",
    "print(X.shape)\n",
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate $w^*$ ~ $P_{true}$, where $P_{true} := \\mathcal{N}(0, \\lambda I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 0. 0. 0. 0.]\n",
      " [0. 5. 0. 0. 0.]\n",
      " [0. 0. 5. 0. 0.]\n",
      " [0. 0. 0. 5. 0.]\n",
      " [0. 0. 0. 0. 5.]]\n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n"
     ]
    }
   ],
   "source": [
    "# ground truth weight distribution P:\n",
    "diag_lambda = 5 # ground truth for P diags\n",
    "diag = np.zeros([dim, dim], dtype=np.float32)\n",
    "for i in range(len(diag)):\n",
    "    diag[i, i] = diag_lambda\n",
    "print(diag)\n",
    "# MSE weight\n",
    "w_star = np.random.multivariate_normal([0]*dim, diag, 1)[0]\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate y: $n$ labels ~ $X^T \\cdot w^* + \\epsilon$. with $\\epsilon$  ~ $\\mathcal{N}(0, I)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise add to y\n",
    "epsilon = np.random.normal(0, 1, n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.72517815 -1.86011148 -1.91710548 -1.39267423 -0.28523969 -1.55699121\n",
      "  2.74346011 -4.57253723 -7.19923035 -4.08182988]\n"
     ]
    }
   ],
   "source": [
    "# noisy labels\n",
    "y = np.dot(X, w_star) + epsilon\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "5\n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(w_star)\n",
    "\n",
    "# groundtruth parameter for P\n",
    "print(diag_lambda)\n",
    "# groundtruth parameter for Q\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is a simple implementation using equation just above section 3.1 in the $\\href{https://arxiv.org/abs/1703.11008}{paper}$ used in the lecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Simple:\n",
    "    def __init__(self, n_samples, dimension, delta, learning_rate=0.001, momentum=0.9):\n",
    "        self.d = dimension\n",
    "        self.m = n_samples\n",
    "        self.delta = delta # delta represents belief\n",
    "        self.optimizer = tf.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "        self._build_model()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.weight = tf.Variable(dtype=tf.float32, name=\"weights\", shape=self.d, initial_value=tf.zeros(shape=self.d), trainable=True)\n",
    "        self.bias = tf.Variable(dtype=tf.float32, name=\"bias\", shape=1, initial_value=tf.zeros(shape=1), trainable=True)\n",
    "        # define learnable distribution Q\n",
    "        self.s = tf.Variable(dtype=tf.float32, name='learnable_diag', shape=self.d, initial_value=tf.ones(shape=self.d), trainable=True)\n",
    "        self.Q = tfd.MultivariateNormalDiag(loc=self.weight, scale_diag=self.s*self.s)\n",
    "        # define P as fixed distribution\n",
    "        self.lamda = [3.0]\n",
    "        self.P = tfd.MultivariateNormalDiag(loc=tf.zeros(self.d), scale_diag=tf.tile(self.lamda, [self.d]))\n",
    "        self.trainable_variables = [self.weight, self.bias, self.s]\n",
    "        \n",
    "    def _sample_si(self):\n",
    "        return np.random.multivariate_normal([.0]*self.d, np.identity(self.d, dtype=np.float32), 1)[0]\n",
    "        \n",
    "    def compute_loss(self, predictions, labels):\n",
    "        empirical_loss = tf.nn.l2_loss(predictions-labels)\n",
    "        KL_divergence = tfd.kl_divergence(distribution_a=self.Q, distribution_b=self.P) # compute KL(Q||P)\n",
    "        RE_loss = (KL_divergence + tf.math.log(self.m/self.delta)) / (2*self.m - 2)\n",
    "        loss = empirical_loss + tf.math.sqrt(RE_loss)\n",
    "        return empirical_loss, loss\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        # apply sgd described in paper: section 3.2\n",
    "        new_weight = self.weight + tf.multiply(self._sample_si(), self.s)\n",
    "        self.scores = tf.reduce_sum(tf.multiply(inputs, new_weight)) + self.bias\n",
    "        return self.scores\n",
    "    \n",
    "    def train(self, dataset, epoch=3, print_step_loss=False):\n",
    "        its = itertools.tee(dataset, epoch)\n",
    "        for e in range(epoch):\n",
    "            _loss = 0\n",
    "            _empirical_loss = 0\n",
    "            for x, y in its[e]:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    scores = self.predict(x)\n",
    "                    empi_loss, loss = self.compute_loss(scores, y)\n",
    "                    _loss += loss\n",
    "                    _empirical_loss += empi_loss\n",
    "                    if print_step_loss:\n",
    "                        print(loss.numpy())\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            print(\"epoch %d ----total loss: %f ---- empirical loss: %f ----\" % (e, _loss/self.m, _empirical_loss/self.m))\n",
    "            self._print_stats()\n",
    "            \n",
    "    def _print_stats(self):\n",
    "        print(\"diag s: \" + str((self.s * self.s).numpy()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 ----total loss: 3.655353 ---- empirical loss: 3.413587 ----\n",
      "diag s: [0.99623805 0.9535061  0.91742533 0.9207604  0.6110957 ]\n",
      "epoch 1 ----total loss: 3.276746 ---- empirical loss: 3.034504 ----\n",
      "diag s: [1.0243871 0.9325476 0.87739   1.0357976 0.1197383]\n",
      "epoch 2 ----total loss: 2.882580 ---- empirical loss: 2.639571 ----\n",
      "diag s: [0.99349666 0.9014705  0.8696092  0.7952672  0.25136375]\n",
      "epoch 3 ----total loss: 2.609730 ---- empirical loss: 2.365815 ----\n",
      "diag s: [0.9644141  0.9125296  0.8947794  0.81660473 0.08865781]\n",
      "epoch 4 ----total loss: 2.438571 ---- empirical loss: 2.193716 ----\n",
      "diag s: [0.9592499  0.8971777  0.8792443  0.82848495 0.06263303]\n",
      "epoch 5 ----total loss: 2.160573 ---- empirical loss: 1.914661 ----\n",
      "diag s: [0.958031   0.9159454  0.8719184  0.9949781  0.00781012]\n",
      "epoch 6 ----total loss: 2.104427 ---- empirical loss: 1.857488 ----\n",
      "diag s: [0.94934094 0.9320908  0.85064715 1.0609933  0.09336017]\n",
      "epoch 7 ----total loss: 1.986336 ---- empirical loss: 1.738313 ----\n",
      "diag s: [0.9407103  0.9307411  0.9261447  1.0360605  0.01881618]\n",
      "epoch 8 ----total loss: 1.897370 ---- empirical loss: 1.648257 ----\n",
      "diag s: [0.9287756  0.9286001  0.9352663  1.0856817  0.01477583]\n",
      "epoch 9 ----total loss: 1.761611 ---- empirical loss: 1.511351 ----\n",
      "diag s: [0.9303049  0.94584894 0.8996906  1.1329424  0.06746649]\n",
      "epoch 10 ----total loss: 1.870839 ---- empirical loss: 1.619340 ----\n",
      "diag s: [0.96453637 0.907607   0.90405613 0.92900616 0.02760845]\n",
      "epoch 11 ----total loss: 1.642617 ---- empirical loss: 1.389863 ----\n",
      "diag s: [1.0001919e+00 9.1506946e-01 8.3689851e-01 1.0015532e+00 5.0947398e-05]\n",
      "epoch 12 ----total loss: 1.701820 ---- empirical loss: 1.447729 ----\n",
      "diag s: [1.0063080e+00 9.1629410e-01 8.2322615e-01 7.7456510e-01 1.6197425e-04]\n",
      "epoch 13 ----total loss: 1.590361 ---- empirical loss: 1.334949 ----\n",
      "diag s: [0.99355966 0.91035223 0.7971411  0.6714404  0.00712672]\n",
      "epoch 14 ----total loss: 1.433254 ---- empirical loss: 1.176662 ----\n",
      "diag s: [0.977122   0.9036894  0.7476732  0.7592507  0.01708852]\n",
      "epoch 15 ----total loss: 1.480204 ---- empirical loss: 1.222387 ----\n",
      "diag s: [0.9695542  0.9100795  0.743419   0.63986564 0.00378354]\n",
      "epoch 16 ----total loss: 1.404570 ---- empirical loss: 1.145522 ----\n",
      "diag s: [0.95902294 0.9068034  0.7407611  0.5570861  0.00122197]\n",
      "epoch 17 ----total loss: 1.360680 ---- empirical loss: 1.100391 ----\n",
      "diag s: [0.9683519  0.9232945  0.70558333 0.49121296 0.0012299 ]\n",
      "epoch 18 ----total loss: 1.378600 ---- empirical loss: 1.117074 ----\n",
      "diag s: [9.7946012e-01 9.1648513e-01 6.5569758e-01 3.5190380e-01 6.4893597e-06]\n",
      "epoch 19 ----total loss: 1.300031 ---- empirical loss: 1.037312 ----\n",
      "diag s: [9.6786511e-01 9.1012168e-01 6.2907350e-01 2.7974761e-01 8.1740646e-04]\n",
      "epoch 20 ----total loss: 1.215646 ---- empirical loss: 0.951755 ----\n",
      "diag s: [9.6296734e-01 9.0022749e-01 6.4310074e-01 2.7768183e-01 6.1187375e-04]\n",
      "epoch 21 ----total loss: 1.253659 ---- empirical loss: 0.988634 ----\n",
      "diag s: [0.9465348  0.89584035 0.6144791  0.19238123 0.00891849]\n",
      "epoch 22 ----total loss: 1.173121 ---- empirical loss: 0.906958 ----\n",
      "diag s: [9.3881845e-01 8.9508241e-01 6.1333644e-01 1.6289704e-01 3.2897314e-04]\n",
      "epoch 23 ----total loss: 1.128634 ---- empirical loss: 0.861427 ----\n",
      "diag s: [0.93577135 0.89145494 0.58966345 0.18415262 0.00216674]\n",
      "epoch 24 ----total loss: 1.142001 ---- empirical loss: 0.873781 ----\n",
      "diag s: [0.9260267  0.87800735 0.5872129  0.15803355 0.00110604]\n",
      "epoch 25 ----total loss: 1.118759 ---- empirical loss: 0.849540 ----\n",
      "diag s: [0.9216485  0.8726948  0.57343554 0.14807543 0.00308026]\n",
      "epoch 26 ----total loss: 1.077405 ---- empirical loss: 0.807194 ----\n",
      "diag s: [0.9117824  0.8658912  0.57277304 0.1523847  0.00373406]\n",
      "epoch 27 ----total loss: 1.063172 ---- empirical loss: 0.792006 ----\n",
      "diag s: [0.9143027  0.8683183  0.5734545  0.12892991 0.00999685]\n",
      "epoch 28 ----total loss: 1.059139 ---- empirical loss: 0.787058 ----\n",
      "diag s: [9.2038304e-01 8.4927422e-01 5.6464094e-01 1.2766592e-01 7.6910038e-04]\n",
      "epoch 29 ----total loss: 1.037565 ---- empirical loss: 0.764569 ----\n",
      "diag s: [0.9103779  0.85827285 0.5440802  0.11320815 0.00348233]\n",
      "epoch 30 ----total loss: 1.026193 ---- empirical loss: 0.752379 ----\n",
      "diag s: [0.9006656  0.8458465  0.52925014 0.12289125 0.00335931]\n",
      "epoch 31 ----total loss: 1.005166 ---- empirical loss: 0.730538 ----\n",
      "diag s: [0.9049358  0.8488229  0.53461796 0.10294909 0.01125995]\n",
      "epoch 32 ----total loss: 1.002676 ---- empirical loss: 0.727311 ----\n",
      "diag s: [0.8927047  0.8349823  0.5301173  0.10300257 0.0016663 ]\n",
      "epoch 33 ----total loss: 1.000977 ---- empirical loss: 0.724862 ----\n",
      "diag s: [0.8956729  0.8319958  0.50262547 0.08920152 0.01278195]\n",
      "epoch 34 ----total loss: 0.973146 ---- empirical loss: 0.696288 ----\n",
      "diag s: [0.8962527  0.8485339  0.51515275 0.06731986 0.01147431]\n",
      "epoch 35 ----total loss: 0.964234 ---- empirical loss: 0.686692 ----\n",
      "diag s: [0.88521814 0.8587529  0.5085097  0.07364476 0.00358565]\n",
      "epoch 36 ----total loss: 0.989819 ---- empirical loss: 0.711639 ----\n",
      "diag s: [8.7899184e-01 8.6377120e-01 4.9413693e-01 6.0650747e-02 4.1996795e-04]\n",
      "epoch 37 ----total loss: 0.990509 ---- empirical loss: 0.711646 ----\n",
      "diag s: [0.86193085 0.8486666  0.4768577  0.04264202 0.00470477]\n",
      "epoch 38 ----total loss: 0.961190 ---- empirical loss: 0.681717 ----\n",
      "diag s: [8.5850537e-01 8.7233710e-01 4.3999961e-01 3.7792236e-02 4.8254841e-04]\n",
      "epoch 39 ----total loss: 0.956463 ---- empirical loss: 0.676401 ----\n",
      "diag s: [0.85457844 0.8690322  0.42119503 0.0355527  0.0062643 ]\n",
      "epoch 40 ----total loss: 0.934347 ---- empirical loss: 0.653706 ----\n",
      "diag s: [0.84866595 0.8701104  0.40167415 0.03782457 0.02247632]\n",
      "epoch 41 ----total loss: 0.974267 ---- empirical loss: 0.693105 ----\n",
      "diag s: [0.83442914 0.860311   0.3736012  0.03732676 0.00819985]\n",
      "epoch 42 ----total loss: 0.926820 ---- empirical loss: 0.645074 ----\n",
      "diag s: [0.83002865 0.85243714 0.3688502  0.03626804 0.01059667]\n",
      "epoch 43 ----total loss: 0.948260 ---- empirical loss: 0.666060 ----\n",
      "diag s: [0.83224916 0.8404923  0.35662538 0.02716571 0.00955068]\n",
      "epoch 44 ----total loss: 0.918549 ---- empirical loss: 0.635858 ----\n",
      "diag s: [0.83915377 0.83943605 0.34430757 0.02592041 0.02692602]\n",
      "epoch 45 ----total loss: 0.929067 ---- empirical loss: 0.645922 ----\n",
      "diag s: [0.8261956  0.8552623  0.36059615 0.02046403 0.00150666]\n",
      "epoch 46 ----total loss: 0.938528 ---- empirical loss: 0.654926 ----\n",
      "diag s: [0.81884253 0.8400214  0.3543265  0.01010464 0.00673502]\n",
      "epoch 47 ----total loss: 0.920903 ---- empirical loss: 0.636868 ----\n",
      "diag s: [8.2101005e-01 8.4485006e-01 3.3119464e-01 2.2936536e-02 2.2025337e-04]\n",
      "epoch 48 ----total loss: 0.896942 ---- empirical loss: 0.612514 ----\n",
      "diag s: [0.8257479  0.8418224  0.34347218 0.02421388 0.01421032]\n",
      "epoch 49 ----total loss: 0.906407 ---- empirical loss: 0.621583 ----\n",
      "diag s: [0.81542104 0.8467761  0.32206836 0.01754363 0.04895245]\n",
      "epoch 50 ----total loss: 0.949309 ---- empirical loss: 0.664094 ----\n",
      "diag s: [0.82307255 0.82918936 0.3024246  0.00892908 0.01356261]\n",
      "epoch 51 ----total loss: 0.923672 ---- empirical loss: 0.638109 ----\n",
      "diag s: [8.1802928e-01 8.2079738e-01 3.1067395e-01 4.5524398e-04 1.4515073e-03]\n",
      "epoch 52 ----total loss: 0.926304 ---- empirical loss: 0.640396 ----\n",
      "diag s: [8.1163037e-01 7.9945332e-01 3.0613229e-01 7.8716985e-04 4.2039808e-03]\n",
      "epoch 53 ----total loss: 0.917115 ---- empirical loss: 0.630883 ----\n",
      "diag s: [8.088250e-01 7.890860e-01 2.984823e-01 6.580871e-07 4.599931e-03]\n",
      "epoch 54 ----total loss: 0.919059 ---- empirical loss: 0.632521 ----\n",
      "diag s: [8.1084871e-01 7.7444226e-01 2.8436455e-01 6.6132670e-05 5.7396656e-03]\n",
      "epoch 55 ----total loss: 0.890505 ---- empirical loss: 0.603700 ----\n",
      "diag s: [7.9631227e-01 7.6868993e-01 2.8868949e-01 4.9378449e-04 4.3247167e-02]\n",
      "epoch 56 ----total loss: 0.891858 ---- empirical loss: 0.604782 ----\n",
      "diag s: [7.9278487e-01 7.7649456e-01 2.8592429e-01 4.0384097e-05 4.8877995e-02]\n",
      "epoch 57 ----total loss: 0.898960 ---- empirical loss: 0.611621 ----\n",
      "diag s: [7.7954179e-01 7.6653892e-01 2.9502651e-01 2.5090817e-04 5.2107994e-02]\n",
      "epoch 58 ----total loss: 0.895175 ---- empirical loss: 0.607565 ----\n",
      "diag s: [7.7046072e-01 7.5815791e-01 2.8737864e-01 1.8161543e-05 6.6402547e-02]\n",
      "epoch 59 ----total loss: 0.922942 ---- empirical loss: 0.635101 ----\n",
      "diag s: [7.7036077e-01 7.5961596e-01 2.7310807e-01 7.9948586e-06 1.3882015e-02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 60 ----total loss: 0.906198 ---- empirical loss: 0.618124 ----\n",
      "diag s: [7.7021623e-01 7.5025135e-01 2.5929669e-01 5.2441974e-05 1.0213447e-02]\n",
      "epoch 61 ----total loss: 0.911700 ---- empirical loss: 0.623407 ----\n",
      "diag s: [7.5742531e-01 7.5197506e-01 2.5082108e-01 5.4338252e-06 1.6061610e-02]\n",
      "epoch 62 ----total loss: 0.926239 ---- empirical loss: 0.637741 ----\n",
      "diag s: [7.3811185e-01 7.5102127e-01 2.3607901e-01 6.5578548e-05 7.4039580e-04]\n",
      "epoch 63 ----total loss: 0.901651 ---- empirical loss: 0.612938 ----\n",
      "diag s: [7.3107642e-01 7.6906186e-01 2.2763495e-01 4.1164723e-04 2.7640630e-04]\n",
      "epoch 64 ----total loss: 0.908737 ---- empirical loss: 0.619844 ----\n",
      "diag s: [7.2637564e-01 7.6782155e-01 2.1154721e-01 5.8265770e-04 3.6008372e-05]\n",
      "epoch 65 ----total loss: 0.911898 ---- empirical loss: 0.622842 ----\n",
      "diag s: [0.7209343  0.76959854 0.1909479  0.00142279 0.00216507]\n",
      "epoch 66 ----total loss: 0.906938 ---- empirical loss: 0.617722 ----\n",
      "diag s: [7.2246236e-01 7.6516193e-01 1.7722863e-01 1.4377963e-03 2.2416236e-04]\n",
      "epoch 67 ----total loss: 0.897215 ---- empirical loss: 0.607840 ----\n",
      "diag s: [7.238669e-01 7.729639e-01 1.713746e-01 2.085594e-04 6.894376e-04]\n",
      "epoch 68 ----total loss: 0.901619 ---- empirical loss: 0.612090 ----\n",
      "diag s: [7.2390920e-01 7.7198690e-01 1.6851464e-01 9.0602953e-05 6.5219728e-04]\n",
      "epoch 69 ----total loss: 0.889436 ---- empirical loss: 0.599735 ----\n",
      "diag s: [7.2828358e-01 7.7123326e-01 1.6896124e-01 2.1057719e-04 5.2581178e-03]\n",
      "epoch 70 ----total loss: 0.907540 ---- empirical loss: 0.617712 ----\n",
      "diag s: [7.2586429e-01 7.6341742e-01 1.6989493e-01 7.4081734e-04 1.0751835e-03]\n",
      "epoch 71 ----total loss: 0.893095 ---- empirical loss: 0.603170 ----\n",
      "diag s: [7.2450030e-01 7.6618564e-01 1.6927364e-01 5.7431636e-04 2.4366181e-03]\n",
      "epoch 72 ----total loss: 0.915181 ---- empirical loss: 0.625129 ----\n",
      "diag s: [0.70954007 0.76283044 0.14745124 0.00514581 0.00153515]\n",
      "epoch 73 ----total loss: 0.894473 ---- empirical loss: 0.604285 ----\n",
      "diag s: [0.7177948  0.76829374 0.13072784 0.0064085  0.00539416]\n",
      "epoch 74 ----total loss: 0.875070 ---- empirical loss: 0.584777 ----\n",
      "diag s: [0.720879   0.7695615  0.12638403 0.01096493 0.02670402]\n",
      "epoch 75 ----total loss: 0.903740 ---- empirical loss: 0.613366 ----\n",
      "diag s: [0.7187642  0.7625638  0.12819664 0.01125657 0.0247638 ]\n",
      "epoch 76 ----total loss: 0.910385 ---- empirical loss: 0.619879 ----\n",
      "diag s: [0.7151167  0.76262075 0.13032785 0.00658377 0.00448934]\n",
      "epoch 77 ----total loss: 0.901780 ---- empirical loss: 0.611151 ----\n",
      "diag s: [0.7058943  0.7620058  0.13186176 0.00289861 0.00370973]\n",
      "epoch 78 ----total loss: 0.890744 ---- empirical loss: 0.599979 ----\n",
      "diag s: [0.69850904 0.7698656  0.1329882  0.0008723  0.01052677]\n",
      "epoch 79 ----total loss: 0.909425 ---- empirical loss: 0.618556 ----\n",
      "diag s: [6.8812209e-01 7.5837189e-01 1.3136111e-01 7.0857524e-04 5.8657019e-03]\n",
      "epoch 80 ----total loss: 0.899341 ---- empirical loss: 0.608389 ----\n",
      "diag s: [6.8872339e-01 7.5169116e-01 1.2904969e-01 1.6911911e-03 1.6068501e-05]\n",
      "epoch 81 ----total loss: 0.898117 ---- empirical loss: 0.607079 ----\n",
      "diag s: [6.7738014e-01 7.5488561e-01 1.3070542e-01 1.4284633e-05 3.4519690e-08]\n",
      "epoch 82 ----total loss: 0.901198 ---- empirical loss: 0.610080 ----\n",
      "diag s: [6.7796266e-01 7.4445128e-01 1.2633434e-01 2.4557440e-04 2.8968642e-03]\n",
      "epoch 83 ----total loss: 0.910507 ---- empirical loss: 0.619299 ----\n",
      "diag s: [6.6846138e-01 7.4616224e-01 1.0565029e-01 2.4133115e-03 1.5752774e-05]\n",
      "epoch 84 ----total loss: 0.885405 ---- empirical loss: 0.594114 ----\n",
      "diag s: [0.6697109  0.7551457  0.10493471 0.00485308 0.00347213]\n",
      "epoch 85 ----total loss: 0.887511 ---- empirical loss: 0.596143 ----\n",
      "diag s: [6.6847378e-01 7.5611353e-01 1.0963032e-01 1.2435730e-02 2.3131348e-04]\n",
      "epoch 86 ----total loss: 0.908503 ---- empirical loss: 0.617086 ----\n",
      "diag s: [0.6471155  0.75279695 0.09800345 0.01514448 0.01560979]\n",
      "epoch 87 ----total loss: 0.881562 ---- empirical loss: 0.590060 ----\n",
      "diag s: [6.5242487e-01 7.6559401e-01 1.0144298e-01 2.7462615e-02 4.4625237e-09]\n",
      "epoch 88 ----total loss: 0.887558 ---- empirical loss: 0.595990 ----\n",
      "diag s: [6.5535301e-01 7.6588017e-01 1.0125575e-01 3.3037107e-02 3.7303084e-04]\n",
      "epoch 89 ----total loss: 0.903833 ---- empirical loss: 0.612205 ----\n",
      "diag s: [0.64330286 0.75168234 0.09950952 0.04578084 0.00498918]\n",
      "epoch 90 ----total loss: 0.889288 ---- empirical loss: 0.597624 ----\n",
      "diag s: [0.64422137 0.7688428  0.09542134 0.03813254 0.00400365]\n",
      "epoch 91 ----total loss: 0.901361 ---- empirical loss: 0.609653 ----\n",
      "diag s: [0.63590103 0.7591129  0.10519651 0.03040499 0.00084977]\n",
      "epoch 92 ----total loss: 0.880416 ---- empirical loss: 0.588676 ----\n",
      "diag s: [0.638332   0.76471394 0.10819405 0.02479385 0.01395653]\n",
      "epoch 93 ----total loss: 0.892329 ---- empirical loss: 0.600566 ----\n",
      "diag s: [0.6371227  0.7651636  0.10375984 0.02305092 0.01357505]\n",
      "epoch 94 ----total loss: 0.886143 ---- empirical loss: 0.594374 ----\n",
      "diag s: [0.6431468  0.7705956  0.09852654 0.02241551 0.01649625]\n",
      "epoch 95 ----total loss: 0.890513 ---- empirical loss: 0.598748 ----\n",
      "diag s: [0.63544965 0.7724109  0.10178409 0.01840232 0.02215488]\n",
      "epoch 96 ----total loss: 0.895090 ---- empirical loss: 0.603268 ----\n",
      "diag s: [0.6337329  0.76259786 0.10189185 0.00924835 0.0223595 ]\n",
      "epoch 97 ----total loss: 0.909347 ---- empirical loss: 0.617500 ----\n",
      "diag s: [0.63106793 0.7593967  0.10366024 0.00742087 0.00128523]\n",
      "epoch 98 ----total loss: 0.905683 ---- empirical loss: 0.613821 ----\n",
      "diag s: [0.62032855 0.7601629  0.09969155 0.00650712 0.00564713]\n",
      "epoch 99 ----total loss: 0.887396 ---- empirical loss: 0.595501 ----\n",
      "diag s: [0.6234379  0.7558213  0.10575011 0.00294373 0.00657509]\n"
     ]
    }
   ],
   "source": [
    "model = Linear_Simple(n_samples=n_sample, dimension=dim, delta=0.025)\n",
    "model.train(zip(X, y), epoch=100, print_step_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth data generation diag: \n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n",
      "learned s: \n",
      "[0.6234379  0.7558212  0.10575011 0.00294373 0.00657509]\n",
      "ground_truth weights: \n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "learned weights: \n",
      "[-0.00957734  0.40996817 -0.7989632   9.566933   -1.80343   ]\n",
      "ground truth Lambda for P: \n",
      "5\n",
      "pre-fixed Lambda for P: \n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"groundtruth data generation diag: \")\n",
    "print(diag_x)\n",
    "print(\"learned s: \")\n",
    "print((model.s **2) .numpy())\n",
    "print(\"ground_truth weights: \")\n",
    "print(w_star)\n",
    "print(\"learned weights: \")\n",
    "print(model.weight.numpy())\n",
    "print(\"ground truth Lambda for P: \")\n",
    "print(diag_lambda)\n",
    "print(\"pre-fixed Lambda for P: \")\n",
    "print(model.lamda[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is the implementation of eq(4) in original paper. where lambda is also a learnable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Paper(Linear_Simple):\n",
    "    def __init__(self, n_samples, dimension, b, c, delta, learning_rate=0.001, momentum=0.9):\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        super().__init__(n_samples, dimension, delta, learning_rate, momentum)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        super()._build_model()\n",
    "        # re-define prior distribution P    \n",
    "        self.lamda = tf.Variable(dtype=tf.float32, name=\"p_lambda\", shape=1, initial_value=[0.01], trainable=True) # diagnals of prior distribution P\n",
    "        self.P = tfd.MultivariateNormalDiag(loc=tf.zeros(self.d), scale_diag=tf.tile(self.lamda, [self.d]))\n",
    "        self.trainable_variables.append(self.lamda)\n",
    "                \n",
    "    def compute_loss(self, predictions, labels):\n",
    "        empirical_loss = tf.nn.l2_loss(predictions-labels)\n",
    "        KL_divergence = tfd.kl_divergence(distribution_a=self.Q, distribution_b=self.P) # compute KL(Q||P)\n",
    "        RE_loss = (KL_divergence + \\\n",
    "                   2*tf.math.log(tf.clip_by_value(self.b * tf.math.log(self.c/self.lamda), 1e-5, 1e30)) + \\\n",
    "                   tf.math.log(math.pi ** 2 * self.m/(6*self.delta))) / (self.m-1)\n",
    "        loss = empirical_loss + tf.math.sqrt(tf.clip_by_value(RE_loss, 0, 1e30)/2)\n",
    "        return empirical_loss, loss\n",
    "            \n",
    "    def _print_stats(self):\n",
    "        super()._print_stats()\n",
    "        print(\"lambda: %f\" % self.lamda.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 ----total loss: 14.462093 ---- empirical loss: 3.173203 ----\n",
      "diag s: [1.0243852  0.96995956 1.0423746  1.0582445  1.0367427 ]\n",
      "lambda: 0.024215\n",
      "epoch 1 ----total loss: 15.120686 ---- empirical loss: 3.761306 ----\n",
      "diag s: [1.0448482  0.97087973 1.0614083  0.7690095  0.42701343]\n",
      "lambda: 0.036674\n",
      "epoch 2 ----total loss: 14.661912 ---- empirical loss: 3.278697 ----\n",
      "diag s: [1.0522382  1.0089128  1.0471274  0.55457675 0.24691749]\n",
      "lambda: 0.048890\n",
      "epoch 3 ----total loss: 14.879770 ---- empirical loss: 3.510389 ----\n",
      "diag s: [1.0590322  0.9898903  1.0322747  0.39413747 0.02116182]\n",
      "lambda: 0.062361\n",
      "epoch 4 ----total loss: 14.709911 ---- empirical loss: 3.334402 ----\n",
      "diag s: [1.0211291  0.96767324 0.9804744  0.44459367 0.00161812]\n",
      "lambda: 0.080127\n",
      "epoch 5 ----total loss: 14.814976 ---- empirical loss: 3.443497 ----\n",
      "diag s: [1.0082163e+00 9.5659113e-01 8.5268956e-01 3.4474167e-01 1.7074196e-04]\n",
      "lambda: 0.111891\n",
      "epoch 6 ----total loss: 14.703222 ---- empirical loss: 3.338418 ----\n",
      "diag s: [1.020557   0.9815849  0.7884699  0.3915572  0.00985471]\n",
      "lambda: 0.111965\n",
      "epoch 7 ----total loss: 14.691763 ---- empirical loss: 3.322665 ----\n",
      "diag s: [1.0165854  0.971294   0.8217368  0.3348519  0.00366036]\n",
      "lambda: 0.111965\n",
      "epoch 8 ----total loss: 14.697452 ---- empirical loss: 3.329166 ----\n",
      "diag s: [1.0200622  1.0075169  0.82252985 0.2489148  0.00184694]\n",
      "lambda: 0.111965\n",
      "epoch 9 ----total loss: 14.612345 ---- empirical loss: 3.250555 ----\n",
      "diag s: [1.002977   1.0123738  0.8410111  0.32698154 0.04916254]\n",
      "lambda: 0.111965\n",
      "epoch 10 ----total loss: 14.718043 ---- empirical loss: 3.357522 ----\n",
      "diag s: [1.0044235  0.979147   0.8427877  0.26483306 0.07585881]\n",
      "lambda: 0.111965\n",
      "epoch 11 ----total loss: 14.704381 ---- empirical loss: 3.340710 ----\n",
      "diag s: [1.0459484  0.98920274 0.7503163  0.29912618 0.02014745]\n",
      "lambda: 0.111965\n",
      "epoch 12 ----total loss: 14.557443 ---- empirical loss: 3.187410 ----\n",
      "diag s: [1.0639389  1.0219699  0.7644058  0.45274213 0.03839575]\n",
      "lambda: 0.111965\n",
      "epoch 13 ----total loss: 14.759844 ---- empirical loss: 3.388755 ----\n",
      "diag s: [1.0748297  1.0259311  0.77438843 0.306464   0.00144576]\n",
      "lambda: 0.111965\n",
      "epoch 14 ----total loss: 14.713370 ---- empirical loss: 3.348788 ----\n",
      "diag s: [1.0223812  1.0556816  0.70022213 0.31979033 0.00836901]\n",
      "lambda: 0.111965\n",
      "epoch 15 ----total loss: 14.761550 ---- empirical loss: 3.391537 ----\n",
      "diag s: [0.99743104 1.0781269  0.6693524  0.24578382 0.00386079]\n",
      "lambda: 0.111965\n",
      "epoch 16 ----total loss: 14.740355 ---- empirical loss: 3.373406 ----\n",
      "diag s: [1.0028714e+00 1.0517644e+00 6.6397375e-01 1.7479004e-01 7.3114534e-05]\n",
      "lambda: 0.111965\n",
      "epoch 17 ----total loss: 14.750480 ---- empirical loss: 3.381737 ----\n",
      "diag s: [0.9876243  1.0475351  0.5967797  0.10827912 0.01920676]\n",
      "lambda: 0.111965\n",
      "epoch 18 ----total loss: 14.672700 ---- empirical loss: 3.307745 ----\n",
      "diag s: [0.9808641  1.0913057  0.5788201  0.1322211  0.00176623]\n",
      "lambda: 0.111965\n",
      "epoch 19 ----total loss: 14.711701 ---- empirical loss: 3.343904 ----\n",
      "diag s: [0.9337212  1.0546043  0.54998434 0.12497281 0.03614247]\n",
      "lambda: 0.111965\n",
      "epoch 20 ----total loss: 14.723873 ---- empirical loss: 3.361944 ----\n",
      "diag s: [0.95751905 1.0144888  0.50573534 0.06968088 0.12344746]\n",
      "lambda: 0.111965\n",
      "epoch 21 ----total loss: 14.758686 ---- empirical loss: 3.394826 ----\n",
      "diag s: [9.4211966e-01 1.0148726e+00 5.1651460e-01 5.8242749e-02 3.4812092e-05]\n",
      "lambda: 0.111965\n",
      "epoch 22 ----total loss: 14.674141 ---- empirical loss: 3.311202 ----\n",
      "diag s: [0.91763633 0.999525   0.5285778  0.11226233 0.00402035]\n",
      "lambda: 0.111965\n",
      "epoch 23 ----total loss: 14.742229 ---- empirical loss: 3.376969 ----\n",
      "diag s: [0.9065235  0.97580135 0.5478808  0.04250089 0.00464206]\n",
      "lambda: 0.111965\n",
      "epoch 24 ----total loss: 14.645110 ---- empirical loss: 3.275143 ----\n",
      "diag s: [0.9126519  0.9578421  0.5465727  0.12046837 0.03234378]\n",
      "lambda: 0.111965\n",
      "epoch 25 ----total loss: 14.682015 ---- empirical loss: 3.315936 ----\n",
      "diag s: [0.9209442  0.9618228  0.58158207 0.09967522 0.05445224]\n",
      "lambda: 0.111965\n",
      "epoch 26 ----total loss: 14.711167 ---- empirical loss: 3.344137 ----\n",
      "diag s: [0.89545566 0.98952454 0.5415763  0.1509615  0.02420538]\n",
      "lambda: 0.111965\n",
      "epoch 27 ----total loss: 14.753329 ---- empirical loss: 3.384277 ----\n",
      "diag s: [0.89725953 0.94465387 0.4760253  0.13499281 0.00605757]\n",
      "lambda: 0.111965\n",
      "epoch 28 ----total loss: 14.745519 ---- empirical loss: 3.377935 ----\n",
      "diag s: [8.8285553e-01 9.0175587e-01 5.2667683e-01 4.9880985e-02 2.6982065e-04]\n",
      "lambda: 0.111965\n",
      "epoch 29 ----total loss: 14.714778 ---- empirical loss: 3.344924 ----\n",
      "diag s: [0.860424   0.8711081  0.4911417  0.03493461 0.02739141]\n",
      "lambda: 0.111965\n",
      "epoch 30 ----total loss: 14.630598 ---- empirical loss: 3.264627 ----\n",
      "diag s: [0.8800626  0.88162094 0.48028934 0.08769949 0.06309152]\n",
      "lambda: 0.111965\n",
      "epoch 31 ----total loss: 14.701071 ---- empirical loss: 3.337801 ----\n",
      "diag s: [0.85995597 0.88218045 0.48320147 0.10164843 0.0193359 ]\n",
      "lambda: 0.111965\n",
      "epoch 32 ----total loss: 14.618783 ---- empirical loss: 3.254500 ----\n",
      "diag s: [0.8941563  0.89902645 0.53803617 0.15609218 0.00600942]\n",
      "lambda: 0.111965\n",
      "epoch 33 ----total loss: 14.663279 ---- empirical loss: 3.298236 ----\n",
      "diag s: [0.8333985  0.89695853 0.5496053  0.23962833 0.00558296]\n",
      "lambda: 0.111965\n",
      "epoch 34 ----total loss: 14.595102 ---- empirical loss: 3.234488 ----\n",
      "diag s: [0.84931344 0.8794022  0.5396172  0.43196672 0.00517463]\n",
      "lambda: 0.111965\n",
      "epoch 35 ----total loss: 14.764941 ---- empirical loss: 3.396062 ----\n",
      "diag s: [0.838584   0.873849   0.5041758  0.2844478  0.05663532]\n",
      "lambda: 0.111965\n",
      "epoch 36 ----total loss: 14.752722 ---- empirical loss: 3.383626 ----\n",
      "diag s: [0.81545764 0.8582595  0.49346167 0.21218653 0.0113732 ]\n",
      "lambda: 0.111965\n",
      "epoch 37 ----total loss: 14.737931 ---- empirical loss: 3.366121 ----\n",
      "diag s: [0.81459904 0.8572636  0.46939692 0.12692305 0.00177513]\n",
      "lambda: 0.111965\n",
      "epoch 38 ----total loss: 14.600266 ---- empirical loss: 3.235060 ----\n",
      "diag s: [0.79599446 0.88214934 0.49844068 0.2734684  0.01090416]\n",
      "lambda: 0.111965\n",
      "epoch 39 ----total loss: 14.688921 ---- empirical loss: 3.322084 ----\n",
      "diag s: [8.0873251e-01 8.4615254e-01 4.8967645e-01 3.0588025e-01 2.8490133e-04]\n",
      "lambda: 0.111965\n",
      "epoch 40 ----total loss: 14.652604 ---- empirical loss: 3.287925 ----\n",
      "diag s: [0.7791693  0.8669073  0.50209445 0.3497994  0.01322909]\n",
      "lambda: 0.111965\n",
      "epoch 41 ----total loss: 14.754753 ---- empirical loss: 3.380582 ----\n",
      "diag s: [0.7679676  0.8777843  0.4651892  0.2682451  0.00547686]\n",
      "lambda: 0.111965\n",
      "epoch 42 ----total loss: 14.588392 ---- empirical loss: 3.220928 ----\n",
      "diag s: [0.7712972  0.83998543 0.428692   0.48512837 0.01238318]\n",
      "lambda: 0.111965\n",
      "epoch 43 ----total loss: 14.728580 ---- empirical loss: 3.367436 ----\n",
      "diag s: [0.75733703 0.80707395 0.35153598 0.4985106  0.01559972]\n",
      "lambda: 0.111965\n",
      "epoch 44 ----total loss: 14.673169 ---- empirical loss: 3.315330 ----\n",
      "diag s: [0.7737022  0.80719465 0.33518046 0.558211   0.02745399]\n",
      "lambda: 0.111965\n",
      "epoch 45 ----total loss: 14.745123 ---- empirical loss: 3.373346 ----\n",
      "diag s: [7.4053687e-01 8.3685696e-01 3.0102465e-01 4.3628213e-01 4.3695001e-04]\n",
      "lambda: 0.111965\n",
      "epoch 46 ----total loss: 14.607378 ---- empirical loss: 3.242143 ----\n",
      "diag s: [0.74238646 0.8563469  0.31917018 0.56013453 0.01635776]\n",
      "lambda: 0.111965\n",
      "epoch 47 ----total loss: 14.733743 ---- empirical loss: 3.365454 ----\n",
      "diag s: [7.6500678e-01 8.4234208e-01 3.3923602e-01 4.0460175e-01 6.0999944e-05]\n",
      "lambda: 0.111965\n",
      "epoch 48 ----total loss: 14.788810 ---- empirical loss: 3.421023 ----\n",
      "diag s: [7.6867622e-01 8.2999521e-01 3.4984741e-01 2.4038307e-01 3.7093923e-05]\n",
      "lambda: 0.111965\n",
      "epoch 49 ----total loss: 14.704951 ---- empirical loss: 3.337633 ----\n",
      "diag s: [7.8360254e-01 7.9156160e-01 3.1570020e-01 2.5385195e-01 1.6951903e-04]\n",
      "lambda: 0.111965\n",
      "epoch 50 ----total loss: 14.682251 ---- empirical loss: 3.315807 ----\n",
      "diag s: [8.0170506e-01 8.1247389e-01 3.1948403e-01 2.1627182e-01 2.6922564e-06]\n",
      "lambda: 0.111965\n",
      "epoch 51 ----total loss: 14.677831 ---- empirical loss: 3.313190 ----\n",
      "diag s: [8.1521481e-01 7.9600352e-01 2.7376178e-01 2.6824021e-01 9.5897019e-05]\n",
      "lambda: 0.111965\n",
      "epoch 52 ----total loss: 14.622650 ---- empirical loss: 3.259437 ----\n",
      "diag s: [0.8088823  0.77512294 0.26035824 0.41637906 0.02023503]\n",
      "lambda: 0.111965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 53 ----total loss: 14.689903 ---- empirical loss: 3.323797 ----\n",
      "diag s: [0.79361385 0.7716567  0.27124175 0.34702963 0.09714367]\n",
      "lambda: 0.111965\n",
      "epoch 54 ----total loss: 14.658734 ---- empirical loss: 3.301307 ----\n",
      "diag s: [0.7838603  0.77097636 0.27328548 0.33627555 0.18235673]\n",
      "lambda: 0.111965\n",
      "epoch 55 ----total loss: 14.718010 ---- empirical loss: 3.349350 ----\n",
      "diag s: [0.7560097  0.78009814 0.27347857 0.38088134 0.00342036]\n",
      "lambda: 0.111965\n",
      "epoch 56 ----total loss: 14.644016 ---- empirical loss: 3.282423 ----\n",
      "diag s: [0.73358405 0.75605416 0.3025157  0.30455548 0.14970613]\n",
      "lambda: 0.111965\n",
      "epoch 57 ----total loss: 14.750640 ---- empirical loss: 3.389775 ----\n",
      "diag s: [0.7467659  0.73872507 0.29301426 0.28415388 0.06428477]\n",
      "lambda: 0.111965\n",
      "epoch 58 ----total loss: 14.609859 ---- empirical loss: 3.245651 ----\n",
      "diag s: [0.8003574  0.7510757  0.30130693 0.40499204 0.00636972]\n",
      "lambda: 0.111965\n",
      "epoch 59 ----total loss: 14.726833 ---- empirical loss: 3.356665 ----\n",
      "diag s: [0.8034949  0.73101443 0.28004912 0.3340039  0.03754795]\n",
      "lambda: 0.111965\n",
      "epoch 60 ----total loss: 14.654117 ---- empirical loss: 3.286242 ----\n",
      "diag s: [0.80351454 0.7341198  0.26600832 0.41472924 0.01280356]\n",
      "lambda: 0.111965\n",
      "epoch 61 ----total loss: 14.728237 ---- empirical loss: 3.372132 ----\n",
      "diag s: [0.82483554 0.7086054  0.24094725 0.43093884 0.00342534]\n",
      "lambda: 0.111965\n",
      "epoch 62 ----total loss: 14.679714 ---- empirical loss: 3.312888 ----\n",
      "diag s: [0.8092067  0.7127045  0.19936882 0.46422717 0.00135492]\n",
      "lambda: 0.111965\n",
      "epoch 63 ----total loss: 14.660998 ---- empirical loss: 3.296613 ----\n",
      "diag s: [0.82803476 0.6987034  0.19768931 0.48691842 0.01272959]\n",
      "lambda: 0.111965\n",
      "epoch 64 ----total loss: 14.596058 ---- empirical loss: 3.230844 ----\n",
      "diag s: [7.9043227e-01 6.9747406e-01 1.7560332e-01 6.8330425e-01 2.6126264e-04]\n",
      "lambda: 0.111965\n",
      "epoch 65 ----total loss: 14.914362 ---- empirical loss: 3.545171 ----\n",
      "diag s: [0.7796762  0.6839777  0.16808008 0.27587825 0.0125312 ]\n",
      "lambda: 0.111965\n",
      "epoch 66 ----total loss: 14.638386 ---- empirical loss: 3.277042 ----\n",
      "diag s: [7.7494425e-01 6.7487627e-01 1.7354904e-01 3.7225738e-01 2.8813967e-05]\n",
      "lambda: 0.111965\n",
      "epoch 67 ----total loss: 14.684501 ---- empirical loss: 3.320867 ----\n",
      "diag s: [0.75450397 0.67746675 0.15408693 0.4003981  0.000863  ]\n",
      "lambda: 0.111965\n",
      "epoch 68 ----total loss: 14.727714 ---- empirical loss: 3.361800 ----\n",
      "diag s: [0.73176336 0.65222484 0.16274816 0.3106645  0.01298653]\n",
      "lambda: 0.111965\n",
      "epoch 69 ----total loss: 14.774681 ---- empirical loss: 3.405876 ----\n",
      "diag s: [0.7618541  0.6285269  0.1376128  0.15085258 0.01812091]\n",
      "lambda: 0.111965\n",
      "epoch 70 ----total loss: 14.682422 ---- empirical loss: 3.323478 ----\n",
      "diag s: [0.7505806  0.6506721  0.1464443  0.13871326 0.00543138]\n",
      "lambda: 0.111965\n",
      "epoch 71 ----total loss: 14.697884 ---- empirical loss: 3.331478 ----\n",
      "diag s: [0.7252319  0.63391846 0.14074421 0.15502065 0.03000967]\n",
      "lambda: 0.111965\n",
      "epoch 72 ----total loss: 14.735967 ---- empirical loss: 3.368909 ----\n",
      "diag s: [0.670832   0.63197297 0.13537094 0.10693862 0.03801188]\n",
      "lambda: 0.111965\n",
      "epoch 73 ----total loss: 14.725904 ---- empirical loss: 3.357359 ----\n",
      "diag s: [0.6863963  0.63037586 0.11908127 0.08501013 0.00218154]\n",
      "lambda: 0.111965\n",
      "epoch 74 ----total loss: 14.660152 ---- empirical loss: 3.296557 ----\n",
      "diag s: [0.657025   0.61227363 0.11189827 0.16045935 0.02601587]\n",
      "lambda: 0.111965\n",
      "epoch 75 ----total loss: 14.741081 ---- empirical loss: 3.371711 ----\n",
      "diag s: [6.6742837e-01 6.2932599e-01 1.1999349e-01 8.1024431e-02 6.8082925e-05]\n",
      "lambda: 0.111965\n",
      "epoch 76 ----total loss: 14.741838 ---- empirical loss: 3.377374 ----\n",
      "diag s: [6.7131889e-01 6.0794562e-01 1.1528915e-01 9.8959981e-03 1.3515640e-04]\n",
      "lambda: 0.111965\n",
      "epoch 77 ----total loss: 14.667235 ---- empirical loss: 3.305107 ----\n",
      "diag s: [0.63966525 0.6108636  0.1147843  0.00337711 0.0739377 ]\n",
      "lambda: 0.111965\n",
      "epoch 78 ----total loss: 14.681893 ---- empirical loss: 3.324307 ----\n",
      "diag s: [0.6037327  0.6226504  0.12827529 0.04562007 0.06401651]\n",
      "lambda: 0.111965\n",
      "epoch 79 ----total loss: 14.710018 ---- empirical loss: 3.347292 ----\n",
      "diag s: [0.5687826  0.6208531  0.1336776  0.00119492 0.08138494]\n",
      "lambda: 0.111965\n",
      "epoch 80 ----total loss: 14.699829 ---- empirical loss: 3.330686 ----\n",
      "diag s: [5.3808558e-01 6.0250956e-01 1.3309331e-01 1.6422142e-04 4.3267988e-02]\n",
      "lambda: 0.111965\n",
      "epoch 81 ----total loss: 14.729190 ---- empirical loss: 3.360768 ----\n",
      "diag s: [0.54881674 0.60109144 0.1112522  0.02643907 0.006085  ]\n",
      "lambda: 0.111965\n",
      "epoch 82 ----total loss: 14.700824 ---- empirical loss: 3.334193 ----\n",
      "diag s: [0.5752283  0.5894471  0.12194121 0.00935983 0.03133834]\n",
      "lambda: 0.111965\n",
      "epoch 83 ----total loss: 14.700428 ---- empirical loss: 3.333005 ----\n",
      "diag s: [0.55973744 0.56396365 0.1035565  0.07480981 0.00535443]\n",
      "lambda: 0.111965\n",
      "epoch 84 ----total loss: 14.679119 ---- empirical loss: 3.311789 ----\n",
      "diag s: [0.54941803 0.54517275 0.10375675 0.02976487 0.0522674 ]\n",
      "lambda: 0.111965\n",
      "epoch 85 ----total loss: 14.712666 ---- empirical loss: 3.352662 ----\n",
      "diag s: [0.56881297 0.5403017  0.10171901 0.06408828 0.0277939 ]\n",
      "lambda: 0.111965\n",
      "epoch 86 ----total loss: 14.688699 ---- empirical loss: 3.319474 ----\n",
      "diag s: [5.5472946e-01 5.0469810e-01 8.7352388e-02 1.1693043e-01 4.7785635e-04]\n",
      "lambda: 0.111965\n",
      "epoch 87 ----total loss: 14.663068 ---- empirical loss: 3.297569 ----\n",
      "diag s: [0.5662434  0.510735   0.08855679 0.1553427  0.01517071]\n",
      "lambda: 0.111965\n",
      "epoch 88 ----total loss: 14.680972 ---- empirical loss: 3.316076 ----\n",
      "diag s: [0.57195324 0.48342732 0.1050209  0.1708925  0.00140406]\n",
      "lambda: 0.111965\n",
      "epoch 89 ----total loss: 14.752532 ---- empirical loss: 3.384369 ----\n",
      "diag s: [0.5737713  0.47979113 0.08181305 0.09302142 0.01068685]\n",
      "lambda: 0.111965\n",
      "epoch 90 ----total loss: 14.691961 ---- empirical loss: 3.323644 ----\n",
      "diag s: [0.54649955 0.46555904 0.07390593 0.07533018 0.06714293]\n",
      "lambda: 0.111965\n",
      "epoch 91 ----total loss: 14.664826 ---- empirical loss: 3.300852 ----\n",
      "diag s: [0.54455566 0.4779471  0.06793986 0.14086981 0.03852982]\n",
      "lambda: 0.111965\n",
      "epoch 92 ----total loss: 14.630874 ---- empirical loss: 3.265476 ----\n",
      "diag s: [0.584048   0.49725217 0.06523845 0.17107807 0.03959843]\n",
      "lambda: 0.111965\n",
      "epoch 93 ----total loss: 14.595975 ---- empirical loss: 3.232142 ----\n",
      "diag s: [0.580063   0.4715391  0.06286355 0.31105366 0.05960423]\n",
      "lambda: 0.111965\n",
      "epoch 94 ----total loss: 14.736815 ---- empirical loss: 3.376731 ----\n",
      "diag s: [0.5814195  0.47842214 0.06402988 0.26157257 0.06792182]\n",
      "lambda: 0.111965\n",
      "epoch 95 ----total loss: 14.686808 ---- empirical loss: 3.320885 ----\n",
      "diag s: [0.56276244 0.4585281  0.06947523 0.32321224 0.00263321]\n",
      "lambda: 0.111965\n",
      "epoch 96 ----total loss: 14.703218 ---- empirical loss: 3.337460 ----\n",
      "diag s: [0.5442488  0.47171012 0.05676949 0.33503988 0.00341296]\n",
      "lambda: 0.111965\n",
      "epoch 97 ----total loss: 14.581529 ---- empirical loss: 3.213647 ----\n",
      "diag s: [0.51983374 0.4873266  0.05257504 0.47637913 0.03982299]\n",
      "lambda: 0.111965\n",
      "epoch 98 ----total loss: 14.764874 ---- empirical loss: 3.397323 ----\n",
      "diag s: [0.48802903 0.47829607 0.05508938 0.38418674 0.01220742]\n",
      "lambda: 0.111965\n",
      "epoch 99 ----total loss: 14.697046 ---- empirical loss: 3.333185 ----\n",
      "diag s: [0.47962907 0.48603508 0.06361027 0.38601834 0.00095246]\n",
      "lambda: 0.111965\n"
     ]
    }
   ],
   "source": [
    "model = Linear_Paper(n_samples=n_sample, dimension=dim, b=100, c=0.1, delta=0.025, learning_rate=0.001)\n",
    "model.train(zip(X, y), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth data generation diag: \n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n",
      "learned s: \n",
      "[0.47962904 0.48603502 0.06361027 0.3860183  0.00095246]\n",
      "ground_truth weights: \n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "learned weights: \n",
      "[ 0.00241566  0.00484082 -0.01352639  0.20059681 -0.25651398]\n",
      "ground truth Lambda for P: \n",
      "5\n",
      "learned Lambda for P: \n",
      "[0.11196508]\n"
     ]
    }
   ],
   "source": [
    "print(\"groundtruth data generation diag: \")\n",
    "print(diag_x)\n",
    "print(\"learned s: \")\n",
    "print((model.s **2) .numpy())\n",
    "print(\"ground_truth weights: \")\n",
    "print(w_star)\n",
    "print(\"learned weights: \")\n",
    "print(model.weight.numpy())\n",
    "print(\"ground truth Lambda for P: \")\n",
    "print(diag_lambda)\n",
    "print(\"learned Lambda for P: \")\n",
    "print(model.lamda.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
