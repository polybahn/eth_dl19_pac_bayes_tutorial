{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.12.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.17.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (0.8.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (3.10.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (0.1.7)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (3.1.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (2.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.11.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.24.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (1.0.8)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (0.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (0.33.6)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow==2.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==2.0) (41.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0) (3.1.1)\n",
      "Requirement already satisfied: h5py in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.0) (2.10.0)\n",
      "Requirement already satisfied: tensorflow_probability in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (0.8.0)\n",
      "Requirement already satisfied: gast<0.3,>=0.2 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow_probability) (0.2.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow_probability) (1.17.2)\n",
      "Requirement already satisfied: decorator in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow_probability) (4.4.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow_probability) (1.12.0)\n",
      "Requirement already satisfied: cloudpickle==1.1.1 in /Users/polybahn/opt/anaconda3/envs/tf2/lib/python3.7/site-packages (from tensorflow_probability) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0\n",
    "!pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In this tutorial, we will implement a simple linear regression model \n",
    "# with PAC Bayesian SGD method\n",
    "import numpy as np\n",
    "np.random.seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "n_sample = 100  # number of samples in training set\n",
    "dim = 5  # dimension of feature vector for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Data preparation\n",
    "# generate X: n samples of dimension d ~ N(0, diag(sigma_1^2, sigma_2^2, ..., sigma_d^2))\n",
    "mean_x = np.zeros(dim, dtype=np.float32)\n",
    "diag_x = np.zeros([dim, dim], dtype=np.float32)\n",
    "for i, sigma in enumerate(np.sort(np.random.rand(dim))):\n",
    "    diag_x[i, i] = sigma ** 2\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "[[-0.07773376  0.01118072  0.06995384  0.10453022 -0.83898637]\n",
      " [-0.08650132 -0.00944478 -0.06890632 -0.2147365  -0.58603212]]\n"
     ]
    }
   ],
   "source": [
    "# generate X\n",
    "X = np.random.multivariate_normal(mean_x, diag_x, n_sample)\n",
    "print(X.shape)\n",
    "print(X[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5. 0. 0. 0. 0.]\n",
      " [0. 5. 0. 0. 0.]\n",
      " [0. 0. 5. 0. 0.]\n",
      " [0. 0. 0. 5. 0.]\n",
      " [0. 0. 0. 0. 5.]]\n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n"
     ]
    }
   ],
   "source": [
    "# generate y: n samples of dimension 1 ~ X^T . w* + epsilon. with epsilon ~ N(0, I)\n",
    "# ground truth weight distribution P:\n",
    "diag_lambda = 5 # ground truth for P diags\n",
    "diag = np.zeros([dim, dim], dtype=np.float32)\n",
    "for i in range(len(diag)):\n",
    "    diag[i, i] = diag_lambda\n",
    "print(diag)\n",
    "# MSE weight\n",
    "w_star = np.random.multivariate_normal([0]*dim, diag, 1)[0]\n",
    "print(w_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise add to y\n",
    "epsilon = np.random.normal(0, 1, n_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.72517815 -1.86011148 -1.91710548 -1.39267423 -0.28523969 -1.55699121\n",
      "  2.74346011 -4.57253723 -7.19923035 -4.08182988]\n"
     ]
    }
   ],
   "source": [
    "# noisy labels\n",
    "y = np.dot(X, w_star) + epsilon\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5)\n",
      "(100,)\n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "5\n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(w_star)\n",
    "\n",
    "# groundtruth parameter for P\n",
    "print(diag_lambda)\n",
    "# groundtruth parameter for Q\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: \n",
    "# define the model\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a simple implementation using equation just above section 3.1 in paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Simple:\n",
    "    def __init__(self, n_samples, dimension, delta, learning_rate=0.001, momentum=0.9):\n",
    "        self.d = dimension\n",
    "        self.m = n_samples\n",
    "        self.delta = delta # delta represents belief\n",
    "        self.optimizer = tf.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "        self._build_model()\n",
    "        \n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.weight = tf.Variable(dtype=tf.float32, name=\"weights\", shape=self.d, initial_value=tf.zeros(shape=self.d), trainable=True)\n",
    "        self.bias = tf.Variable(dtype=tf.float32, name=\"bias\", shape=1, initial_value=tf.zeros(shape=1), trainable=True)\n",
    "        # define learnable distribution Q\n",
    "        self.s = tf.Variable(dtype=tf.float32, name='learnable_diag', shape=self.d, initial_value=tf.ones(shape=self.d), trainable=True)\n",
    "        self.Q = tfd.MultivariateNormalDiag(loc=self.weight, scale_diag=self.s*self.s)\n",
    "        # define P as fixed distribution\n",
    "        self.lamda = [3.0]\n",
    "        self.P = tfd.MultivariateNormalDiag(loc=tf.zeros(self.d), scale_diag=tf.tile(self.lamda, [self.d]))\n",
    "        self.trainable_variables = [self.weight, self.bias, self.s]\n",
    "        \n",
    "    def _sample_phi(self):\n",
    "        return np.random.multivariate_normal([.0]*self.d, np.identity(self.d, dtype=np.float32), 1)[0]\n",
    "        \n",
    "    def compute_loss(self, predictions, labels):\n",
    "        empirical_loss = tf.nn.l2_loss(predictions-labels)\n",
    "        KL_divergence = tfd.kl_divergence(distribution_a=self.Q, distribution_b=self.P) # compute KL(Q||P)\n",
    "        RE_loss = (KL_divergence + tf.math.log(self.m/self.delta)) / (2*self.m - 2)\n",
    "        loss = empirical_loss + tf.math.sqrt(RE_loss)\n",
    "        return loss\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        # apply sgd described in paper: section 3.2\n",
    "        new_weight = self.weight + tf.multiply(self._sample_phi(), self.s)\n",
    "        self.scores = tf.reduce_sum(tf.multiply(inputs, new_weight)) + self.bias\n",
    "        return self.scores\n",
    "    \n",
    "    def train(self, dataset, epoch=3, print_step_loss=False):\n",
    "        its = itertools.tee(dataset, epoch)\n",
    "        for e in range(epoch):\n",
    "            _loss = 0\n",
    "            for x, y in its[e]:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    scores = self.predict(x)\n",
    "                    loss = self.compute_loss(scores, y)\n",
    "                    _loss += loss\n",
    "                    if print_step_loss:\n",
    "                        print(loss.numpy())\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            print(\"average loss at epoch %d is: %f\" % (e, _loss/self.m))\n",
    "            self._print_stats()\n",
    "            \n",
    "    def _print_stats(self):\n",
    "        print(\"diag s: \" + str((self.s * self.s).numpy()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at epoch 0 is: 3.740866\n",
      "diag s: [0.99481803 1.0270623  0.96434236 1.0287461  0.32720926]\n",
      "average loss at epoch 1 is: 3.234305\n",
      "diag s: [1.0229533  1.0139452  0.8806895  0.8765629  0.14445315]\n",
      "average loss at epoch 2 is: 2.917866\n",
      "diag s: [1.0001037  1.0469124  0.81301045 0.7482896  0.14153744]\n",
      "average loss at epoch 3 is: 2.634566\n",
      "diag s: [0.99541485 1.0400496  0.8058162  0.69964737 0.01819591]\n",
      "average loss at epoch 4 is: 2.426518\n",
      "diag s: [0.9915569 1.0181191 0.7514363 0.8257075 0.0139191]\n",
      "average loss at epoch 5 is: 2.391124\n",
      "diag s: [0.9942878  1.0256128  0.7362773  0.6148264  0.01033538]\n",
      "average loss at epoch 6 is: 2.170149\n",
      "diag s: [0.9812073  1.0012332  0.7577909  0.61120325 0.01170056]\n",
      "average loss at epoch 7 is: 2.057967\n",
      "diag s: [0.97988075 0.99544257 0.7636441  0.53251815 0.01911303]\n",
      "average loss at epoch 8 is: 1.962724\n",
      "diag s: [0.9819547  0.98891675 0.7389337  0.4842021  0.0069253 ]\n",
      "average loss at epoch 9 is: 1.825257\n",
      "diag s: [0.98564476 0.98099786 0.74202126 0.4707392  0.01381182]\n",
      "average loss at epoch 10 is: 1.772745\n",
      "diag s: [0.987589   0.95710176 0.730512   0.3860247  0.02788526]\n",
      "average loss at epoch 11 is: 1.657142\n",
      "diag s: [9.9437207e-01 9.6119505e-01 7.3847651e-01 3.9518127e-01 7.5965852e-04]\n",
      "average loss at epoch 12 is: 1.587807\n",
      "diag s: [0.997596  0.9576826 0.6978035 0.3964528 0.0193245]\n",
      "average loss at epoch 13 is: 1.493530\n",
      "diag s: [1.0004742e+00 9.7136092e-01 6.7992294e-01 4.4476250e-01 8.7132602e-04]\n",
      "average loss at epoch 14 is: 1.510012\n",
      "diag s: [9.9666482e-01 9.7558433e-01 6.7293119e-01 3.7854210e-01 1.6848701e-04]\n",
      "average loss at epoch 15 is: 1.455157\n",
      "diag s: [1.0114475  0.96272695 0.63442    0.32149836 0.00302039]\n",
      "average loss at epoch 16 is: 1.380491\n",
      "diag s: [1.0151823  0.96115124 0.6249091  0.27851176 0.00131109]\n",
      "average loss at epoch 17 is: 1.375176\n",
      "diag s: [0.9866921  0.96070874 0.62126416 0.19538853 0.00255007]\n",
      "average loss at epoch 18 is: 1.308829\n",
      "diag s: [9.7547942e-01 9.5154625e-01 6.5714443e-01 1.2056991e-01 2.4352201e-04]\n",
      "average loss at epoch 19 is: 1.246105\n",
      "diag s: [0.97590405 0.95732623 0.6381222  0.12992367 0.00472982]\n",
      "average loss at epoch 20 is: 1.238434\n",
      "diag s: [0.9793557  0.9498818  0.6246871  0.10384601 0.00528222]\n",
      "average loss at epoch 21 is: 1.187991\n",
      "diag s: [9.5875657e-01 9.2862856e-01 6.2957132e-01 1.2518659e-01 8.3069819e-05]\n",
      "average loss at epoch 22 is: 1.166905\n",
      "diag s: [9.4046396e-01 9.3540168e-01 6.4332759e-01 9.4551139e-02 6.9883349e-04]\n",
      "average loss at epoch 23 is: 1.177704\n",
      "diag s: [0.9362592  0.9207821  0.6149927  0.05715664 0.0019339 ]\n",
      "average loss at epoch 24 is: 1.112550\n",
      "diag s: [0.9204128  0.93488336 0.61767477 0.03645358 0.01608982]\n",
      "average loss at epoch 25 is: 1.106477\n",
      "diag s: [0.9148604  0.92752385 0.6288701  0.02228867 0.00828985]\n",
      "average loss at epoch 26 is: 1.098413\n",
      "diag s: [0.90774727 0.9266611  0.6124102  0.01780508 0.00105626]\n",
      "average loss at epoch 27 is: 1.069781\n",
      "diag s: [0.87101763 0.9301092  0.5938354  0.01197272 0.00820384]\n",
      "average loss at epoch 28 is: 1.047068\n",
      "diag s: [8.592136e-01 9.436368e-01 5.929388e-01 8.021617e-03 2.972243e-05]\n",
      "average loss at epoch 29 is: 1.035181\n",
      "diag s: [0.8685395  0.9368969  0.57297146 0.01341945 0.0024098 ]\n",
      "average loss at epoch 30 is: 1.027355\n",
      "diag s: [0.8633021  0.939325   0.5486508  0.01000366 0.00280537]\n",
      "average loss at epoch 31 is: 1.007248\n",
      "diag s: [0.8626165  0.93197805 0.5465095  0.00479085 0.00301854]\n",
      "average loss at epoch 32 is: 1.000861\n",
      "diag s: [8.6189020e-01 9.4398731e-01 5.3266448e-01 5.3944844e-03 4.5125850e-04]\n",
      "average loss at epoch 33 is: 0.985954\n",
      "diag s: [8.6432803e-01 9.3729174e-01 5.3201735e-01 9.4957668e-03 6.7362838e-05]\n",
      "average loss at epoch 34 is: 0.979494\n",
      "diag s: [0.85224277 0.9451308  0.5235304  0.00985496 0.00922607]\n",
      "average loss at epoch 35 is: 0.988097\n",
      "diag s: [0.83174604 0.9261063  0.5228849  0.01102133 0.00153243]\n",
      "average loss at epoch 36 is: 0.929786\n",
      "diag s: [0.841153   0.9281483  0.5436308  0.01505057 0.02661395]\n",
      "average loss at epoch 37 is: 0.964545\n",
      "diag s: [0.83607554 0.9362955  0.54196066 0.01325074 0.0059101 ]\n",
      "average loss at epoch 38 is: 0.950407\n",
      "diag s: [0.8321154  0.9233058  0.5496333  0.01140225 0.00976092]\n",
      "average loss at epoch 39 is: 0.950135\n",
      "diag s: [0.83498895 0.92889774 0.5423904  0.00910799 0.0020356 ]\n",
      "average loss at epoch 40 is: 0.954062\n",
      "diag s: [8.2006603e-01 9.3481350e-01 5.2676111e-01 1.0141927e-02 4.5721693e-04]\n",
      "average loss at epoch 41 is: 0.924609\n",
      "diag s: [0.82134235 0.93339443 0.52156705 0.01859651 0.00926611]\n",
      "average loss at epoch 42 is: 0.924914\n",
      "diag s: [0.8201777  0.93045276 0.5167481  0.02650296 0.02192627]\n",
      "average loss at epoch 43 is: 0.953887\n",
      "diag s: [0.8247789  0.92529666 0.4970128  0.02123808 0.00290333]\n",
      "average loss at epoch 44 is: 0.929617\n",
      "diag s: [0.82876515 0.9282119  0.4835892  0.01802825 0.00278613]\n",
      "average loss at epoch 45 is: 0.908021\n",
      "diag s: [8.3864242e-01 9.1890913e-01 4.9682051e-01 2.5300724e-02 3.4920836e-04]\n",
      "average loss at epoch 46 is: 0.951862\n",
      "diag s: [0.83225137 0.9081911  0.461661   0.02191784 0.0028626 ]\n",
      "average loss at epoch 47 is: 0.928375\n",
      "diag s: [0.8206819  0.9103702  0.4403483  0.0214003  0.00736915]\n",
      "average loss at epoch 48 is: 0.919963\n",
      "diag s: [0.82428986 0.90525436 0.43605348 0.01292939 0.0052445 ]\n",
      "average loss at epoch 49 is: 0.932537\n",
      "diag s: [8.1946540e-01 8.9090836e-01 4.2634681e-01 1.9804118e-02 2.6781319e-04]\n",
      "average loss at epoch 50 is: 0.915068\n",
      "diag s: [8.0709237e-01 9.0282607e-01 4.1809589e-01 2.4266211e-02 1.6051283e-04]\n",
      "average loss at epoch 51 is: 0.907148\n",
      "diag s: [0.80589646 0.89738953 0.41724473 0.02032775 0.02765283]\n",
      "average loss at epoch 52 is: 0.912395\n",
      "diag s: [8.0755681e-01 8.8750297e-01 4.3986014e-01 1.6827248e-02 6.5145298e-04]\n",
      "average loss at epoch 53 is: 0.914109\n",
      "diag s: [7.9820162e-01 8.9083850e-01 4.4277194e-01 1.0975524e-02 2.6082691e-07]\n",
      "average loss at epoch 54 is: 0.918706\n",
      "diag s: [0.7980932  0.8697438  0.4278213  0.01588652 0.006987  ]\n",
      "average loss at epoch 55 is: 0.924725\n",
      "diag s: [0.7900832  0.87323505 0.4108572  0.00852394 0.00106589]\n",
      "average loss at epoch 56 is: 0.915569\n",
      "diag s: [0.78561544 0.855776   0.4046844  0.00750524 0.00545362]\n",
      "average loss at epoch 57 is: 0.915021\n",
      "diag s: [7.7387166e-01 8.4825516e-01 3.9903349e-01 7.5948252e-03 3.7284204e-04]\n",
      "average loss at epoch 58 is: 0.907299\n",
      "diag s: [0.781136   0.846992   0.39340144 0.00799991 0.00803167]\n",
      "average loss at epoch 59 is: 0.912288\n",
      "diag s: [7.8735751e-01 8.4362203e-01 3.6769676e-01 1.2604631e-02 3.0354224e-04]\n",
      "average loss at epoch 60 is: 0.917598\n",
      "diag s: [0.7771267  0.8303363  0.3681309  0.00510849 0.00421685]\n",
      "average loss at epoch 61 is: 0.901870\n",
      "diag s: [0.7730961  0.8318911  0.36598557 0.00235583 0.00375487]\n",
      "average loss at epoch 62 is: 0.906075\n",
      "diag s: [7.7506500e-01 8.2048291e-01 3.6279473e-01 1.4706332e-03 5.0072436e-04]\n",
      "average loss at epoch 63 is: 0.896296\n",
      "diag s: [7.6697201e-01 8.2113373e-01 3.7430590e-01 3.7524290e-04 4.5627484e-04]\n",
      "average loss at epoch 64 is: 0.907847\n",
      "diag s: [7.6911622e-01 8.2438380e-01 3.6318249e-01 4.1711837e-04 9.2058079e-05]\n",
      "average loss at epoch 65 is: 0.918304\n",
      "diag s: [7.6577967e-01 8.0711114e-01 3.3743632e-01 4.7945182e-04 8.3562016e-05]\n",
      "average loss at epoch 66 is: 0.921129\n",
      "diag s: [7.5464541e-01 7.8080153e-01 3.3056405e-01 1.3815946e-04 7.9144239e-03]\n",
      "average loss at epoch 67 is: 0.901312\n",
      "diag s: [7.4986762e-01 7.8130132e-01 3.2056734e-01 2.7265633e-04 1.3777475e-04]\n",
      "average loss at epoch 68 is: 0.902654\n",
      "diag s: [7.4897391e-01 7.7884781e-01 3.0596015e-01 2.4995650e-04 9.5773963e-03]\n",
      "average loss at epoch 69 is: 0.914805\n",
      "diag s: [0.7339591  0.7798845  0.28459415 0.00093381 0.00150882]\n",
      "average loss at epoch 70 is: 0.887508\n",
      "diag s: [0.743816   0.79033977 0.2784793  0.00152791 0.00306042]\n",
      "average loss at epoch 71 is: 0.911547\n",
      "diag s: [0.7287383  0.7794129  0.27426606 0.01146993 0.01239994]\n",
      "average loss at epoch 72 is: 0.918294\n",
      "diag s: [0.72577715 0.7702628  0.2594492  0.00579919 0.00240869]\n",
      "average loss at epoch 73 is: 0.892313\n",
      "diag s: [0.7387434  0.7743876  0.25399557 0.0061562  0.0030308 ]\n",
      "average loss at epoch 74 is: 0.887683\n",
      "diag s: [0.7369124  0.76680374 0.2550478  0.00665693 0.02405561]\n",
      "average loss at epoch 75 is: 0.916800\n",
      "diag s: [0.7278247  0.7635559  0.24822788 0.00408151 0.00426813]\n",
      "average loss at epoch 76 is: 0.910807\n",
      "diag s: [0.7202672  0.7606811  0.23908958 0.00452518 0.0020031 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at epoch 77 is: 0.884558\n",
      "diag s: [7.2449881e-01 7.6319820e-01 2.4901277e-01 6.8923654e-03 3.8762728e-04]\n",
      "average loss at epoch 78 is: 0.905282\n",
      "diag s: [7.1288460e-01 7.6202625e-01 2.3925944e-01 8.3799008e-03 1.0823606e-05]\n",
      "average loss at epoch 79 is: 0.908053\n",
      "diag s: [0.69839317 0.74737465 0.23390509 0.01390431 0.01213733]\n",
      "average loss at epoch 80 is: 0.894784\n",
      "diag s: [0.68348575 0.7316585  0.23313184 0.01008356 0.03857528]\n",
      "average loss at epoch 81 is: 0.901653\n",
      "diag s: [0.6922533  0.723738   0.239395   0.00522361 0.0289592 ]\n",
      "average loss at epoch 82 is: 0.900916\n",
      "diag s: [0.6813152  0.7329053  0.24039073 0.00298153 0.00352013]\n",
      "average loss at epoch 83 is: 0.890880\n",
      "diag s: [0.6697032  0.73917454 0.23671174 0.00117891 0.0318952 ]\n",
      "average loss at epoch 84 is: 0.915721\n",
      "diag s: [0.65111697 0.7221709  0.21864381 0.00447042 0.02369864]\n",
      "average loss at epoch 85 is: 0.909448\n",
      "diag s: [0.6452284  0.72083414 0.21047889 0.01075998 0.00112018]\n",
      "average loss at epoch 86 is: 0.894349\n",
      "diag s: [6.4663523e-01 7.2509241e-01 2.0336120e-01 1.3531014e-02 1.7015771e-05]\n",
      "average loss at epoch 87 is: 0.894438\n",
      "diag s: [0.6495     0.7161683  0.1985902  0.01142816 0.00904895]\n",
      "average loss at epoch 88 is: 0.896894\n",
      "diag s: [0.6474227  0.71295387 0.19488305 0.01895596 0.0045673 ]\n",
      "average loss at epoch 89 is: 0.912350\n",
      "diag s: [0.63769156 0.70460314 0.18463413 0.01066423 0.00417792]\n",
      "average loss at epoch 90 is: 0.901468\n",
      "diag s: [0.6273539  0.7033329  0.18688773 0.00660782 0.00481306]\n",
      "average loss at epoch 91 is: 0.897770\n",
      "diag s: [0.60795885 0.70525664 0.18522078 0.00757432 0.01770324]\n",
      "average loss at epoch 92 is: 0.908445\n",
      "diag s: [0.61115223 0.6991836  0.18125658 0.00306063 0.00073695]\n",
      "average loss at epoch 93 is: 0.896466\n",
      "diag s: [0.5929615  0.70933044 0.17124546 0.00687528 0.01141267]\n",
      "average loss at epoch 94 is: 0.889619\n",
      "diag s: [0.5945391  0.704586   0.16735752 0.00789704 0.01400833]\n",
      "average loss at epoch 95 is: 0.886116\n",
      "diag s: [0.59175307 0.69837594 0.17421903 0.00615132 0.01950325]\n",
      "average loss at epoch 96 is: 0.904063\n",
      "diag s: [0.58625    0.6896909  0.17395541 0.00793607 0.00962352]\n",
      "average loss at epoch 97 is: 0.909415\n",
      "diag s: [5.7744634e-01 6.8620127e-01 1.6328686e-01 4.5667491e-03 1.6837561e-05]\n",
      "average loss at epoch 98 is: 0.896222\n",
      "diag s: [5.7923692e-01 6.7795765e-01 1.6240323e-01 4.8235641e-03 6.0310506e-04]\n",
      "average loss at epoch 99 is: 0.877795\n",
      "diag s: [0.5850705  0.6827301  0.16667593 0.01240806 0.00622486]\n",
      "groundtruth data generation diag: \n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n",
      "learned s: \n",
      "[0.5850705  0.6827301  0.16667593 0.01240806 0.00622486]\n",
      "ground_truth weights: \n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "learned weights: \n",
      "[ 4.9536594e-04  3.9592439e-01 -8.1628710e-01  9.5732365e+00\n",
      " -1.8067658e+00]\n"
     ]
    }
   ],
   "source": [
    "model = Linear_Simple(n_samples=n_sample, dimension=dim, delta=0.025)\n",
    "model.train(zip(X, y), epoch=100, print_step_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth data generation diag: \n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n",
      "learned s: \n",
      "[0.6603121  0.775492   0.30869764 0.11568683 0.02556862]\n",
      "ground_truth weights: \n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "learned weights: \n",
      "[ 0.00251557  0.00429379 -0.01429341  0.19932954 -0.25943637]\n",
      "ground truth Lambda for P: \n",
      "5\n",
      "learned Lambda for P: \n",
      "[0.11213199]\n"
     ]
    }
   ],
   "source": [
    "print(\"groundtruth data generation diag: \")\n",
    "print(diag_x)\n",
    "print(\"learned s: \")\n",
    "print((model.s **2) .numpy())\n",
    "print(\"ground_truth weights: \")\n",
    "print(w_star)\n",
    "print(\"learned weights: \")\n",
    "print(model.weight.numpy())\n",
    "print(\"ground truth Lambda for P: \")\n",
    "print(diag_lambda)\n",
    "print(\"learned Lambda for P: \")\n",
    "print(model.lamda.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below is the implementation of eq(4) in original paper. where lambda is also a learnable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Paper(Linear_Simple):\n",
    "    def __init__(self, n_samples, dimension, b, c, delta, learning_rate=0.001, momentum=0.9):\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        super().__init__(n_samples, dimension, delta, learning_rate, momentum)\n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.weight = tf.Variable(dtype=tf.float32, name=\"weights\", shape=self.d, initial_value=tf.zeros(shape=self.d), trainable=True)\n",
    "        self.bias = tf.Variable(dtype=tf.float32, name=\"bias\", shape=1, initial_value=tf.zeros(shape=1), trainable=True)\n",
    "        # define learnable distribution Q\n",
    "        self.s = tf.Variable(dtype=tf.float32, name='learnable_diag', shape=self.d, initial_value=tf.ones(shape=self.d), trainable=True)\n",
    "        self.Q = tfd.MultivariateNormalDiag(loc=self.weight, scale_diag=self.s*self.s)\n",
    "        # define prior distribution P    \n",
    "        self.lamda = tf.Variable(dtype=tf.float32, name=\"p_lambda\", shape=1, initial_value=[0.01], trainable=True) # diagnals of prior distribution P\n",
    "        self.P = tfd.MultivariateNormalDiag(loc=tf.zeros(self.d), scale_diag=tf.tile(self.lamda, [self.d]))\n",
    "        self.trainable_variables = [self.weight, self.bias, self.s, self.lamda]\n",
    "                \n",
    "    def compute_loss(self, predictions, labels):\n",
    "        empirical_loss = tf.nn.l2_loss(predictions-labels)\n",
    "        KL_divergence = tfd.kl_divergence(distribution_a=self.Q, distribution_b=self.P) # compute KL(Q||P)\n",
    "        RE_loss = (KL_divergence + \\\n",
    "                   2*tf.math.log(tf.clip_by_value(self.b * tf.math.log(self.c/self.lamda), 1e-5, 1e30)) + \\\n",
    "                   tf.math.log(math.pi ** 2 * self.m/(6*self.delta))) / (self.m-1)\n",
    "        loss = empirical_loss + tf.math.sqrt(tf.clip_by_value(RE_loss, 0, 1e30)/2)\n",
    "        return loss\n",
    "            \n",
    "    def _print_stats(self):\n",
    "        super()._print_stats()\n",
    "        print(\"lambda: %f\" % self.lamda.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at epoch 0 is: 14.684502\n",
      "diag s: [1.0091044  0.9613919  0.9781083  1.1547221  0.36836565]\n",
      "lambda: 0.024212\n",
      "average loss at epoch 1 is: 14.818404\n",
      "diag s: [1.0228788  0.9312968  0.8993562  1.0948617  0.18609247]\n",
      "lambda: 0.036660\n",
      "average loss at epoch 2 is: 14.886151\n",
      "diag s: [1.0102277  0.9305457  0.85511667 0.8747031  0.03690664]\n",
      "lambda: 0.048871\n",
      "average loss at epoch 3 is: 14.641424\n",
      "diag s: [0.98379284 0.9136573  0.8367183  0.94107395 0.00554069]\n",
      "lambda: 0.062324\n",
      "average loss at epoch 4 is: 14.532815\n",
      "diag s: [1.0071161  0.8837219  0.87317795 1.191427   0.0121667 ]\n",
      "lambda: 0.080074\n",
      "average loss at epoch 5 is: 14.786790\n",
      "diag s: [1.0150102  0.87750614 0.8720314  0.98344296 0.00385936]\n",
      "lambda: 0.187454\n",
      "average loss at epoch 6 is: 14.778208\n",
      "diag s: [1.0208317  0.84528416 0.8301078  0.795103   0.00484626]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 7 is: 14.811518\n",
      "diag s: [0.9848045  0.8472985  0.75783265 0.5014059  0.09085104]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 8 is: 14.786369\n",
      "diag s: [1.0012656  0.8519435  0.7214033  0.42064297 0.00665074]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 9 is: 14.641415\n",
      "diag s: [0.9997219  0.8399373  0.71689004 0.4752916  0.00384496]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 10 is: 14.732117\n",
      "diag s: [0.99944204 0.7948307  0.66031414 0.44875658 0.01729562]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 11 is: 14.659150\n",
      "diag s: [1.028222   0.78160226 0.6289162  0.5561486  0.0015218 ]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 12 is: 14.769735\n",
      "diag s: [0.9927676  0.75432897 0.6229407  0.38037792 0.09745649]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 13 is: 14.666501\n",
      "diag s: [0.95842123 0.7593837  0.65666586 0.4030892  0.04233729]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 14 is: 14.672077\n",
      "diag s: [0.9503254  0.7548017  0.6695224  0.47662485 0.00274886]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 15 is: 14.522865\n",
      "diag s: [0.9574003  0.78848517 0.666558   0.63498515 0.04449186]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 16 is: 14.597874\n",
      "diag s: [0.9343794  0.80369043 0.65620875 0.7625097  0.19791172]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 17 is: 14.602001\n",
      "diag s: [0.9346563  0.7963645  0.7593282  0.747855   0.07907219]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 18 is: 14.551257\n",
      "diag s: [0.9183029  0.7861418  0.7921625  0.8625338  0.09910521]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 19 is: 14.565815\n",
      "diag s: [0.90520936 0.81388384 0.7297267  1.035127   0.12669384]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 20 is: 14.793110\n",
      "diag s: [0.96122175 0.7852219  0.7570679  0.8443656  0.0450352 ]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 21 is: 14.592443\n",
      "diag s: [1.0273968 0.7853645 0.6920685 0.9421908 0.0503228]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 22 is: 14.886392\n",
      "diag s: [1.0538064  0.7898147  0.64948946 0.5729376  0.04023601]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 23 is: 14.752545\n",
      "diag s: [1.0395696e+00 7.8783303e-01 6.5427911e-01 4.6649417e-01 4.2644286e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 24 is: 14.538928\n",
      "diag s: [1.0211071  0.8045053  0.56592375 0.7320636  0.05861358]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 25 is: 14.737804\n",
      "diag s: [1.010708   0.82078874 0.54494953 0.6026394  0.11056948]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 26 is: 14.659416\n",
      "diag s: [0.99044603 0.8116318  0.5623735  0.61667466 0.02217034]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 27 is: 14.764507\n",
      "diag s: [1.0099083  0.8365709  0.5208755  0.48798373 0.01147017]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 28 is: 14.794559\n",
      "diag s: [1.0501974  0.8320575  0.439549   0.33715466 0.0015365 ]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 29 is: 14.707545\n",
      "diag s: [1.0217535  0.8534245  0.41374692 0.30270633 0.0252049 ]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 30 is: 14.692034\n",
      "diag s: [1.0140895  0.83387345 0.44897622 0.3097039  0.00484027]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 31 is: 14.791115\n",
      "diag s: [1.0110390e+00 8.0898541e-01 4.3214077e-01 1.5516156e-01 1.1254126e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 32 is: 14.646111\n",
      "diag s: [0.99266416 0.8135918  0.46120787 0.13938935 0.03737112]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 33 is: 14.656525\n",
      "diag s: [0.98092586 0.8137618  0.45744076 0.21528189 0.00818321]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 34 is: 14.696924\n",
      "diag s: [0.9775643  0.7943914  0.4709571  0.20575969 0.00148121]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 35 is: 14.689570\n",
      "diag s: [0.96859545 0.7823615  0.5032228  0.20004302 0.0043052 ]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 36 is: 14.678781\n",
      "diag s: [0.9351562  0.7705427  0.52398384 0.17179824 0.10482062]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 37 is: 14.695576\n",
      "diag s: [0.95173    0.79105604 0.5511263  0.10928022 0.09257557]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 38 is: 14.733962\n",
      "diag s: [0.9593465  0.772547   0.5115532  0.08467804 0.00986823]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 39 is: 14.700260\n",
      "diag s: [0.96165866 0.8082474  0.47414312 0.02945899 0.00313266]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 40 is: 14.668779\n",
      "diag s: [0.9425685  0.778549   0.46644223 0.10763983 0.00697856]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 41 is: 14.686453\n",
      "diag s: [0.9655935  0.80952203 0.5153998  0.0372646  0.01167919]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 42 is: 14.632835\n",
      "diag s: [0.92550826 0.81138736 0.54804116 0.06834189 0.03642127]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 43 is: 14.732378\n",
      "diag s: [0.9457404  0.8119137  0.51507664 0.01688142 0.01581771]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 44 is: 14.698795\n",
      "diag s: [0.9367656  0.8324925  0.51963246 0.00433    0.00522363]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 45 is: 14.637826\n",
      "diag s: [9.3356603e-01 8.4934962e-01 5.4209387e-01 6.5787144e-02 6.5082750e-05]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 46 is: 14.750446\n",
      "diag s: [0.92350143 0.85196626 0.4936471  0.03109985 0.03631167]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 47 is: 14.641219\n",
      "diag s: [0.8946945  0.84689957 0.48010728 0.00532776 0.16090934]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 48 is: 14.719501\n",
      "diag s: [9.0806234e-01 9.0649128e-01 4.3145347e-01 3.1752017e-04 3.6916692e-02]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 49 is: 14.717107\n",
      "diag s: [0.90900296 0.8904678  0.40495336 0.00327473 0.00815269]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 50 is: 14.673384\n",
      "diag s: [9.2150599e-01 8.9185017e-01 3.8044932e-01 6.6624465e-04 5.9684655e-03]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 51 is: 14.774981\n",
      "diag s: [0.8966123  0.8716145  0.342748   0.01180391 0.00139208]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 52 is: 14.680966\n",
      "diag s: [0.866949   0.8844819  0.34692416 0.02053887 0.00167818]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 53 is: 14.695139\n",
      "diag s: [0.8318326  0.86705    0.36718625 0.00350527 0.06776517]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 54 is: 14.654634\n",
      "diag s: [0.8446954  0.8632051  0.40953872 0.06599878 0.00100854]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 55 is: 14.717996\n",
      "diag s: [0.8320327  0.8555569  0.41502815 0.0094208  0.04363615]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 56 is: 14.741439\n",
      "diag s: [0.79936355 0.8249886  0.40592685 0.10012618 0.00909857]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 57 is: 14.679418\n",
      "diag s: [0.80219924 0.8431115  0.3738101  0.05493807 0.06171795]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 58 is: 14.754015\n",
      "diag s: [0.81009007 0.8199139  0.36152586 0.01843772 0.02945487]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 59 is: 14.722387\n",
      "diag s: [7.7933538e-01 8.0990541e-01 3.3426496e-01 5.9434589e-02 5.8547215e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 60 is: 14.693804\n",
      "diag s: [0.75578636 0.7898324  0.2863087  0.12833607 0.00860479]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 61 is: 14.670994\n",
      "diag s: [7.4539590e-01 7.8816521e-01 2.9285368e-01 1.7323765e-01 1.6415495e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 62 is: 14.745482\n",
      "diag s: [7.34094501e-01 7.52436697e-01 2.91358173e-01 1.11555494e-01\n",
      " 7.40931719e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 63 is: 14.671621\n",
      "diag s: [7.0185256e-01 7.4303699e-01 2.9283547e-01 1.7430514e-01 2.2482370e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 64 is: 14.608912\n",
      "diag s: [0.708983   0.71946824 0.2993291  0.348152   0.00171634]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 65 is: 14.668275\n",
      "diag s: [0.69381535 0.720829   0.32305142 0.34957403 0.00140038]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 66 is: 14.709206\n",
      "diag s: [0.7047635  0.7195013  0.3142702  0.2687758  0.04792434]\n",
      "lambda: 0.188014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss at epoch 67 is: 14.731799\n",
      "diag s: [0.67695194 0.69067705 0.32989895 0.19620535 0.02762379]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 68 is: 14.782635\n",
      "diag s: [0.66901714 0.6617494  0.3204606  0.07292069 0.03477235]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 69 is: 14.705016\n",
      "diag s: [0.6653107  0.6533075  0.28954533 0.09839866 0.03473667]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 70 is: 14.710004\n",
      "diag s: [0.6797719  0.6355406  0.2682696  0.14842682 0.00438321]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 71 is: 14.675573\n",
      "diag s: [0.67735237 0.65159756 0.23825891 0.15025994 0.04352284]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 72 is: 14.623749\n",
      "diag s: [0.6515304  0.6413449  0.20173495 0.28359172 0.06813668]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 73 is: 14.741642\n",
      "diag s: [0.6446236  0.63482434 0.1944143  0.25716364 0.01024378]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 74 is: 14.656399\n",
      "diag s: [6.5702826e-01 6.4490384e-01 1.8049215e-01 3.4789693e-01 4.3491463e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 75 is: 14.656051\n",
      "diag s: [6.2336552e-01 6.5824276e-01 1.7616293e-01 3.9152855e-01 2.0695568e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 76 is: 14.786372\n",
      "diag s: [0.6592427  0.66221267 0.14757352 0.17914237 0.00257439]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 77 is: 14.715511\n",
      "diag s: [0.6447851  0.6501137  0.12792867 0.13045307 0.05634638]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 78 is: 14.350801\n",
      "diag s: [0.65357876 0.64417934 0.12260255 0.26447    0.42514512]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 79 is: 14.807507\n",
      "diag s: [0.66654533 0.6324215  0.0978536  0.29961106 0.21621239]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 80 is: 14.750219\n",
      "diag s: [0.6998815  0.6096799  0.11932573 0.21669266 0.00707063]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 81 is: 14.614050\n",
      "diag s: [0.70367676 0.6206423  0.12108425 0.34699744 0.01664229]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 82 is: 14.758529\n",
      "diag s: [0.69298285 0.5902076  0.1149004  0.28748798 0.04773641]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 83 is: 14.688163\n",
      "diag s: [0.69798756 0.6046591  0.09431407 0.21546476 0.06453439]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 84 is: 14.648635\n",
      "diag s: [7.0247346e-01 6.2553668e-01 1.1924419e-01 2.7918231e-01 2.4122396e-04]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 85 is: 14.729192\n",
      "diag s: [0.69476354 0.63323724 0.08982345 0.26523218 0.0079493 ]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 86 is: 14.760232\n",
      "diag s: [0.69479376 0.62729985 0.06627268 0.16412573 0.02252839]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 87 is: 14.769688\n",
      "diag s: [6.5470314e-01 6.2089962e-01 5.7383899e-02 1.4031252e-01 9.3073149e-06]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 88 is: 14.669873\n",
      "diag s: [0.6203816  0.6282562  0.04109113 0.141181   0.05227936]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 89 is: 14.782055\n",
      "diag s: [0.6043981  0.62666    0.04551554 0.06929014 0.00457829]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 90 is: 14.727084\n",
      "diag s: [0.6188366  0.6003802  0.04449496 0.01496348 0.00540095]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 91 is: 14.705950\n",
      "diag s: [0.6078776  0.5960952  0.02567145 0.00164826 0.02039067]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 92 is: 14.715171\n",
      "diag s: [0.58270204 0.6053928  0.024367   0.01644986 0.00155856]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 93 is: 14.664961\n",
      "diag s: [0.56195915 0.60305405 0.01261382 0.01898592 0.07356147]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 94 is: 14.737887\n",
      "diag s: [0.5388205  0.6006455  0.01910645 0.01265176 0.05579144]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 95 is: 14.634435\n",
      "diag s: [0.5522538  0.58708996 0.02117329 0.00094454 0.07415817]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 96 is: 14.747173\n",
      "diag s: [0.5451554  0.59789634 0.03087771 0.00762832 0.06903828]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 97 is: 14.707262\n",
      "diag s: [0.5657317  0.5870094  0.02693734 0.01323781 0.00465997]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 98 is: 14.706568\n",
      "diag s: [0.5607472  0.5699045  0.02007463 0.00180027 0.00815173]\n",
      "lambda: 0.188014\n",
      "average loss at epoch 99 is: 14.707569\n",
      "diag s: [0.5043025  0.57687074 0.01902749 0.03450254 0.00063787]\n",
      "lambda: 0.188014\n"
     ]
    }
   ],
   "source": [
    "model = Linear_Paper(n_samples=n_sample, dimension=dim, b=100, c=0.1, delta=0.025, learning_rate=0.001)\n",
    "model.train(zip(X, y), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth data generation diag: \n",
      "[[0.0018896  0.         0.         0.         0.        ]\n",
      " [0.         0.00212482 0.         0.         0.        ]\n",
      " [0.         0.         0.01355448 0.         0.        ]\n",
      " [0.         0.         0.         0.06296267 0.        ]\n",
      " [0.         0.         0.         0.         0.45808023]]\n",
      "learned s: \n",
      "[0.5043025  0.57687074 0.01902749 0.03450254 0.00063787]\n",
      "ground_truth weights: \n",
      "[-1.42438729  1.77600611 -1.39529334  8.73640413 -1.69140474]\n",
      "learned weights: \n",
      "[ 0.00248535  0.00475126 -0.01315061  0.19954143 -0.25769135]\n",
      "ground truth Lambda for P: \n",
      "5\n",
      "learned Lambda for P: \n",
      "[0.18801434]\n"
     ]
    }
   ],
   "source": [
    "print(\"groundtruth data generation diag: \")\n",
    "print(diag_x)\n",
    "print(\"learned s: \")\n",
    "print((model.s **2) .numpy())\n",
    "print(\"ground_truth weights: \")\n",
    "print(w_star)\n",
    "print(\"learned weights: \")\n",
    "print(model.weight.numpy())\n",
    "print(\"ground truth Lambda for P: \")\n",
    "print(diag_lambda)\n",
    "print(\"learned Lambda for P: \")\n",
    "print(model.lamda.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
